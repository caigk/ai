# 自然语言处理NLP

!> 学习 **NNLM > Word2Vec > Seq2Seq > Seq2Seq with Attention > Transformer > ElMo > GPT > BERT**

## 一、学习资源

* [Introduction to TensorFlow Text](https://tensorflow.google.cn/text/guide/tf_text_intro)
* [TensorFlow text processing tutorials](https://tensorflow.google.cn/text/tutorials)
* [Lena Voita NLP Course](https://lena-voita.github.io/nlp_course.html) 💯

**论文**

* [A Study on Neural Network Language Modeling](papers/arxiv.org.pdf.1708.07252v1.pdf ':ignore') 201708
* 🌷CBOW | Skip-gram [Efficient Estimation of Word Representations in Vector Space](papers/arxiv.org.pdf.1301.3781v3.pdf ':ignore') 201307
* 🌷CBOW | Skip-gram [Distributed Representations of Words and Phrases and their Compositionality](papers/NIPS-2013-distributed-representations-of-words-and-phrases-and-their-compositionality-Paper.pdf ':ignore')
* 🌷Seq2seq [Sequence to Sequence Learning with Neural Networks](papers/arxiv.org.pdf.1409.3215v3.pdf ':ignore') 201412
* 🌷Transformer [Attention Is All You Need](papers/arxiv.org.pdf.1706.03762v7.pdf ':ignore') 202308
* 🌷GPT [Improving Language Understanding by Generative Pre-Training](papers/language_understanding_paper.pdf ':ignore') 
* 🌷BERT [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](papers/arxiv.org.pdf.1810.04805v2.pdf ':ignore') 201903
* [The Llama 3 Herd of Models](papers/TheLlama3HerdofModels.pdf ':ignore') 202406
* 🌷[TRANSFORMER EXPLAINER: Interactive Learning of Text-Generative Models](papers/arxiv.org.paf.2408.04619v1.pdf ':ignore') 202408

## 二、课前准备

安装环境

```sh

pip install tensorflow  -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install tensorflow-datasets  -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install tensorflow-text  -i https://pypi.tuna.tsinghua.edu.cn/simple

#测试环境
python -c "import tensorflow as tf;print(f'tensorflow-{tf.__version__} keras-{tf.keras.__version__}')"
```

## 三、练习

代码在群中下载

* word2vec.ipynb
* word_embeddings.ipynb
* warmstart_embedding_matrix.ipynb
