# è‡ªç„¶è¯­è¨€å¤„ç†NLP

!> å­¦ä¹  **NNLM > Word2Vec > Seq2Seq > Seq2Seq with Attention > Transformer > ElMo > GPT > BERT**

## ä¸€ã€å­¦ä¹ èµ„æº

* [Introduction to TensorFlow Text](https://tensorflow.google.cn/text/guide/tf_text_intro)
* [TensorFlow text processing tutorials](https://tensorflow.google.cn/text/tutorials)
* [Lena Voita NLP Course](https://lena-voita.github.io/nlp_course.html) ğŸ’¯

**è®ºæ–‡**

* [A Study on Neural Network Language Modeling](papers/arxiv.org.pdf.1708.07252v1.pdf ':ignore')
* ğŸŒ·CBOW | Skip-gram [Efficient Estimation of Word Representations in Vector Space](papers/arxiv.org.pdf.1301.3781v3.pdf ':ignore')
* ğŸŒ·Seq2seq [Sequence to Sequence Learning with Neural Networks](papers/arxiv.org.pdf.1409.3215v3.pdf ':ignore')
* ğŸŒ·Transformer [Attention Is All You Need](papers/arxiv.org.pdf.1706.03762v7.pdf ':ignore')
* ğŸŒ·GPT [Improving Language Understanding by Generative Pre-Training](papers/language_understanding_paper.pdf ':ignore')
* ğŸŒ·BERT [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](papers/arxiv.org.pdf.1810.04805v2.pdf ':ignore')
* [The Llama 3 Herd of Models](papers/TheLlama3HerdofModels.pdf ':ignore')

## äºŒã€è¯¾å‰å‡†å¤‡

å®‰è£…ç¯å¢ƒ

```sh

pip install tensorflow  -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install tensorflow-datasets  -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install tensorflow-text  -i https://pypi.tuna.tsinghua.edu.cn/simple

#æµ‹è¯•ç¯å¢ƒ
python -c "import tensorflow as tf;print(f'tensorflow-{tf.__version__} keras-{tf.keras.__version__}')"
```

## ä¸‰ã€ç»ƒä¹ 

ä»£ç åœ¨ç¾¤ä¸­ä¸‹è½½

* word2vec.ipynb
* word_embeddings.ipynb
* warmstart_embedding_matrix.ipynb
